---
creation date: 2024-02-21
aliases:
  - Q124 all hands demo
---
- Keep the basic flow but change from fashion to coffee shop



This video showcases the Intent based contextual search capabilities we've worked in in Q1 2024. We'll use the USS explorer & CKG demo hub to show these capabilities running, instead of hypothetical mock ups or UI click throughs. 

To start, lets set some context. Imagine a marketer is interested in creating a new asset to post on social media as part of an ongoing campaign to promote a new menu offering at their coffee shop. 

They start with a search for “car detailing” in the search bar, and they first see some related queries like "cafe" and "cafe bakery". Then in main results area, given the somewhat ambiguous query, we see the first set of results is photos of cafes, followed by a few other content types. But since this user wants to post on instagram, they add the term "post". We now understand the intention to create something, and templates jump up in the results, and most of them are square given the popularity of instragram. We also see design types for instagram and facebook posts. 

Let's jump forward and show what the current editor experience would be, once they'd found the right template for them (urn:aaid:sc:VA6C2:434a748c-df84-4525-ae61-ec2b80975b52). This template seems to be advertising some orange infused drink, but our user isn't looking for that kind of content. So they go to look for a more suitable image of coffee. Today you can see that when they click on the media tab, they're greeted with a boilerplate list of collections, which have nothing to do with coffee and now the user needs to make another query and browse through the results to get something useful to replace that orange drink with. 

Jumping back to our Intent based demo hub, we can look at this same template but here we're also seeing some returned contextual photo collections on the left hand side. These collections are driven off of our intent understanding which incorporates a number of factors including the query the user entered, the time of year and location of the user, as well as the template they've selected. 
We see collections like Coffee, and Barista, which are much more useful for replacing the orange drink in the original template. Now the user would simply select one of the images in question and position it appropriately on the canvas.

This not only saves the user time and creates a smoother editor experience, but saves them from needing to think of all the options which might make sense to search for, maybe showing them new ideas to edit into their canvas.

We only looked at photos in the media tab today. But this intent understanding system will power similar experiences across many elements from icons, to fonts, to audio or video. I look forward to sharing more progress as we start rolling out for these other element types in the near future. 