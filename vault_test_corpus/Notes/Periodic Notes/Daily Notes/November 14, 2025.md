---
pageType: daily
aliases:
  - "2025-11-14"
created: "2025-11-14"
---
# [[November 14, 2025]]
# Notes
---





# Meetings 
---

## [[Recommendations Home|Recs]] Standup #meetings 

design asset and icon [[Style Home|style]] [[Recommendations Home|recs]] in [[express]] has an issue where the [[Style Home|style]] isn't persisted to the full results list. The thumbnail is looking great, the full results have no [[Style Home|style]] signal. 

## [[APS]] Review #meetings 
- 2026 deck: https://[[express]].adobe.com/id/urn:aaid:sc:US:061c895a-3857-5c19-8afa-cda687cc38ab?category=search

1. call out [[Recommendations Home|contextual discovery]] for the agent
2. Call out autocomplete re-do as part of semantic rollout 
3. ping peter about whats up w unwanted and preset [[Recommendations Home|recs]] 2

## Tier one concepts review #meetings 

[[Anshul Omar|Anshul]]'s adobe ecosystem concepts in [Excel](https://adobe-my.sharepoint.com/:x:/r/personal/anomar_adobe_com/_layouts/15/doc2.aspx?sourcedoc=%7BF2DA880F-8757-4D18-8AF5-5F52CFD39E30%7D&file=tier1_concepts.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&wdOrigin=APPHOME-WEB.LOGIN%2CAPPHOME-WEB.FILEBROWSER.RECENT&wdPreviousSession=b0c1e19a-f4aa-4ac2-aa57-906a2ee53bba&wdPreviousSessionSrc=AppHomeWeb&ct=1763138036437).
We need to add the [[Intent AI Home|CKG]] service tiering to the excel. 
## Sean Park #meetings/interviewer 

- [[2026 Summer Internships - Ritu's Team]]
verdict: no - would be a good intern on Subhjait's team, not Ritu's. 

### 1. Take me through your resume a little bit. 
EE and CS, AI Eng at Samsung on chatbot, made a KG and semantic.
Said "owner of the product" :^(

Went to MBA for biz skills, started talking about what he's bringing to the company, wearing a suit. Very formal. 
Stiff comms, seems quite smart. 
### 2. What drew you to the Product Management profession?
Decision making about product. Translating the biz need and strat thinks. 
### 3. What draw you to the search and AI space?
Wrote [[RAG]] paper in masters deg. Ranking and LLM. 

### 3A. Quality vs compute tradeoffs
Latency, compute (batt.), model size. 
### 4. Let's do a quick thought exercise, let's say the team has come to you with a new version of an intent understanding model that they feel is ready to ship in the photoshop chat assistant. How would you go about evaluating and shipping it?
Structured approach: 
1. prob def. 
	1. [[Intent AI Home|intent Ai]] may improve User exp
2. MVP
	1. Hypothesis "model may improve CSAT..."
	2. defining the metrics and guardrails
		1. CTR
		2. CSAT
		3. Latency
3. A/B test
	1. tradeoffs accuracy vs latency
	2. "North start metrics"
		1. biz growth?
		2. UX
			1. customer and user metrics - didn't actually say what they were
			2. next best action and accuracy 
**Rollout**
1. Prototype and feedback tech preview
2. roll straight out
	1. roll straight out 
### 5. Any questions for me? 

1. what skills would be most used
	1. trying REALLY hard to sell himself 
2. 


## Armando #meetings/interviewer 

- [[2026 Summer Internships - Ritu's Team]]

verdict: yes!
### 1. Take me through your resume a little bit. 
AI and enterpenureship at wharton, CS deg. CapitolOne PM

1. tech depth
	1. big data migration type of shit
	2. streaming and chaos testing
2. innovating
	1. internal R&D / labs 
		1. Figuring out LLM placement within Cap1
3. getting shit built
	1. shipped the first CapOne AI app (RAG thing)

### 2. What drew you to the Product Management profession?
Liked being technically fluent but not nuts and bolts. Didn't like consulting bc you don't see anything through. Builder, likes ownership. 

### 3. What draw you to the search and AI space?
LLM hype, but fair enough. 
Search and RAG felt like areas where you could prove out the tech and stuff. 
knows the golden lesson of search, garbage in garbage out...

### 4. Let's do a quick thought exercise, let's say the team has come to you with a new version of a model that they feel is ready to ship. How would you go about evaluating and shipping it?

Asked some clarifying quesitons. 

What is the customer problem, use cases, existing KPIs for shit in prod now. 
How the model is performing offline, comparing to baseline. 
shadow input, where it's capturing prompts live but you only run the new system offline. User interviews and hands on reviews. 
Gradual rollout. 
messages needed to get a task done, tool engagemnet is a red harring. Are people taking the rcs from the bot, going for UX success and validating the KPIs with UXR. 
"Signal is signal"

**harm and bias**
UX harm in terms of people sliding down a bad path. What is the tolerance. per use case.Tradeoffs between efficacy and saftey.

### 5. Any questions for me? 
1. mobility within adobe
2. Speed of inno


## Sneha #meetings/interviewer 

Scary 

Wants to work in B2B. 
### 1. Take me through your resume a little bit. 
Comp eng, was at Salesforce. Wanted to be in the B2B space. 
Backendy at first, moved closer to FE over time. 
Building out search and AI in video calling caoching app. 

Wanting to decide what gets built. MBAI eng + MBA. 
### 2. What drew you to the Product Management profession?
tech skills + soft skills
User needs and working cross functionally before writing code. 
Ambiguous problems.
### 3. What draw you to the search and AI space?
Team moved her, but enjoyed the space. Cross team effort for search. Building out the RAG and hybrid search. 
Video call transcripts get embedded, the rag was preexisting and used. 
### 4. Let's do a quick thought exercise, let's say the team has come to you with a new version of a model that they feel is ready to ship. How would you go about evaluating and shipping it?
1. asked about UX and use case here
2. made some boundary calls about users and use cases 

Understanding the use case and the personas. A/B tests, shipping to segments. Biz case definition. 
Then diving into metrics, evaluating via metrics. 

Accuracy and engagement, are people using the new one. Not v familiar with intent. Back and forth before action. 

UXR for the longer horizon stuff, feedback buttons for users. 

**harm and bias**
Looking at the model and pre-training data first, good answer. 
Guardrails and functional checks. 
### 5. Any questions for me? 
1. Fragmentation of tech stack
2. overlap between CC apps
3. Cross app interplay
4. proud at adobe 




