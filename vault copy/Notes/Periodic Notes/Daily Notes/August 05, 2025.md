---
pageType: daily
aliases:
  - "2025-08-05"
created: "2025-08-05"
---
# [[August 05, 2025]]
# Notes
---



# Meetings 
---

## AI Foundations Program #meetings/pgm 

**[[Query Understanding]] Projects**: 
- [[NER & SRL|SRL]] V1.1 model deployed with baseline accuracy computed on golden set
- [[ILUP]] API changes required for new global SRL types - post-processing moving to ILUP
- Discovery Hub integration work ongoing

**[[Core Affinity Framework]] Leaderboard**: Early results show significant outperformance vs base models including GPT-4 mini across various tasks and query lengths.

## Intent daily sync #meetings 
**[[NER & SRL|SRL]]** 
Finalized prompt for generation of training & [[Evaluation|eval]] data. 
~2M rows created with llama, each row then used to create 3-4 variations each. 8M total for training 

## [[NER & SRL|SRL]] Requirements clarification #meetings 
- [Wiki](https://wiki.corp.adobe.com/pages/viewpage.action?spaceKey=adobesearch&title=Intent+Understanding:+SRL+model)
- Decision on name fields: for names, we will rely on the client to resolve `@alias` to a hexID which will will consume in [[NER & SRL|SRL]]. We'll keep the plain text names for internal testing and validation purposes only. 
- DC no longer needs semantic or natural language search for sept release, but we still want to demo these capabilities to them. 


## Alex Hu #meetings/interviewer 

- [[Principal PM Interview Questions]]

Sees a future of self improvements. 
AI impact in creativity, zero creativity in answer…
Human relationships with AI 

Two trends: 
1. Creativity can be truly democratized 
	1. Sees slop balancing out 
	2. Reading or consuming content will shift 
2. How content is displayed will get more multi modal 

RAG scenario

1. Eval
Eval data set, and group of eval stakeholders 
- Translate queries or use cases in data set to build a golden set. Understanding these major use cases translates to the data set. 
Comparing the results against lexical search
Feedback from live users. Cost considerations as well. 
Go / no-go decision is up to leadership.
- In his exp its user feedback based
- Looking at current prod failure cases for lift
Answer is all over the place.
Answer still all over the place. 
Brinigng data to leadership to make the actual call is the core piece. 

1. Ethics and safety 
Consult trust and safety team at adobe. 
- They will also test relevance here? 
Content moderation, and design built in should be shown to them. Building out the problematic set. 
If they still find problematic responses, why would the data set allow this? ID root cause? Size the problem by likelihood of user hitting this response. 
Plow the learnings back into future iterations. 

Testing for users looking to provoke the system. 

1. First, build different user cohorts. 
Rolled out in staggered manner. 
Very eng driven, comes from the tech side always. 

Then figure out the metrics… Explicit feedback signals. Never actually Ids what the metrics are… Keeps going back to measuring turns chatted as KPI. 

Actual metrics, when pushed:
- Clicks
	- Other behaviors to measure… 
	- Didn’t give any 
- Biz metrics 
	- MAU
	- Returning users
- Thumbs up/down
- % of corpus being returned? 

Keeps talking about iding metrics, but lists none. Always in the position of being given. Looking for customer metrics with a Word cloud? 
Comparing with historical data. 

**Influence question:** 
Showing the partner team the risk as well as the opportunity. What’s the impact. Leadership escalation to get more staffing, or push the date. 
Experimentation is a risk here
Is where we are now, acceptable. 
Is the quality of the system today, without improvements, actually not shippable?
Ship commit is committed. 


**Why PM?**
- Finding sweet spot 
- Wanted to look into IDing the right problem to solve. 
As he got into it, he found PM is more about personal curiosity. Are you solving the big problem?
Why build a product at all?
Don’t just complain about things.